/**
 * Generators for DevPlan MCP Server.
 * Creates PROJECT_BRIEF.md, DEVELOPMENT_PLAN.md, and claude.md content.
 *
 * Key principle: Generate detailed scaffolding that Claude Code can follow
 * like "paint-by-numbers" - every subtask has explicit deliverables, files,
 * and success criteria so there's nothing left to imagination.
 */

import type { DevelopmentPlan, Phase, ProjectBrief, TechStack } from "./models";
import { getTemplate, PROJECT_TYPE_TASKS, type PhaseTemplate } from "./templates";

export interface BriefInput {
	name: string;
	projectType: string;
	goal: string;
	targetUsers: string[];
	features: string[];
	techStack?: {
		mustUse?: string[];
		cannotUse?: string[];
	};
	timeline?: string;
	teamSize?: number;
	constraints?: string[];
	niceToHave?: string[];
}

export function createBrief(input: BriefInput): string {
	const mustUse = input.techStack?.mustUse?.map((t) => `- ${t}`).join("\n") || "- (none specified)";
	const cannotUse = input.techStack?.cannotUse?.map((t) => `- ${t}`).join("\n") || "- (none specified)";
	const features = input.features.map((f) => `- ${f}`).join("\n");
	const constraints = input.constraints?.map((c) => `- ${c}`).join("\n") || "- (none specified)";
	const niceToHave = input.niceToHave?.length
		? input.niceToHave.map((f) => `- ${f}`).join("\n")
		: "- (to be determined after MVP)";

	return `# PROJECT_BRIEF.md

## Basic Information

- **Project Name**: ${input.name}
- **Project Type**: ${input.projectType}
- **Primary Goal**: ${input.goal}
- **Target Users**: ${input.targetUsers.join(", ")}
- **Timeline**: ${input.timeline || "Not specified"}
- **Team Size**: ${input.teamSize || 1}

## Functional Requirements

### Key Features (MVP)

${features}

### Nice-to-Have Features (v2)

${niceToHave}

## Technical Constraints

### Must Use

${mustUse}

### Cannot Use

${cannotUse}

## Other Constraints

${constraints}

## Success Criteria

- All MVP features implemented and working
- Code passes linting and type checking
- Test coverage >= 80%
- Documentation complete

---

*Generated by DevPlan MCP Server*
`;
}

export function parseBrief(content: string): ProjectBrief {
	const lines = content.split("\n");

	const extractField = (fieldName: string): string => {
		for (const line of lines) {
			if (line.includes(`**${fieldName}**:`)) {
				const value = line.split(":").slice(1).join(":").trim();
				return value.replace(/\*\*/g, "");
			}
		}
		return "";
	};

	const extractList = (sectionName: string): string[] => {
		const items: string[] = [];
		let inSection = false;

		for (const line of lines) {
			if (line.includes(sectionName)) {
				inSection = true;
				continue;
			}
			if (inSection) {
				if (line.startsWith("##") || line.startsWith("---")) {
					break;
				}
				if (line.startsWith("- ") && !line.includes("(none") && !line.includes("(to be")) {
					items.push(line.substring(2).trim());
				}
			}
		}
		return items;
	};

	return {
		projectName: extractField("Project Name"),
		projectType: extractField("Project Type"),
		primaryGoal: extractField("Primary Goal"),
		targetUsers: extractField("Target Users"),
		timeline: extractField("Timeline"),
		teamSize: extractField("Team Size") || "1",
		keyFeatures: extractList("Key Features"),
		niceToHaveFeatures: extractList("Nice-to-Have"),
		mustUseTech: extractList("Must Use"),
		cannotUseTech: extractList("Cannot Use"),
		successCriteria: extractList("Success Criteria"),
		performanceRequirements: {},
		securityRequirements: {},
		scalabilityRequirements: {},
		availabilityRequirements: {},
		existingKnowledge: [],
		infrastructureAccess: [],
		externalSystems: [],
		dataSources: [],
		dataDestinations: [],
		knownChallenges: [],
		referenceMaterials: [],
		questionsAndClarifications: [],
		useCases: [],
		deliverables: [],
	};
}

export function generateTechStack(brief: ProjectBrief): TechStack {
	const template = getTemplate(brief.projectType);
	const defaults = template.defaultTechStack;

	const cannotUse = brief.cannotUseTech.map((t) => t.toLowerCase());

	const isBlocked = (tech: string | undefined): boolean => {
		if (!tech) return false;
		return cannotUse.some((blocked) => tech.toLowerCase().includes(blocked));
	};

	// Build tech stack respecting constraints
	let language = defaults.language;
	let framework = defaults.framework || "";
	let database = defaults.database || "";
	const testing = defaults.testing || "pytest";
	const linting = defaults.linting || "ruff";
	const typeChecking = defaults.typeChecking || "mypy";
	let deployment = defaults.deployment || "";
	const ciCd = defaults.ciCd || "GitHub Actions";

	if (isBlocked(language)) language = "Python 3.11+";
	if (isBlocked(framework)) framework = "";
	if (isBlocked(database)) database = "";
	if (isBlocked(deployment)) deployment = "";

	// Add must_use items to additional tools
	const additionalTools: Record<string, string> = {};
	brief.mustUseTech.forEach((tech, i) => {
		additionalTools[`must_use_${i}`] = tech;
	});

	return {
		language,
		framework,
		database,
		testing,
		linting,
		typeChecking,
		deployment,
		ciCd,
		additionalTools,
	};
}

export function generatePhases(brief: ProjectBrief): Phase[] {
	const template = getTemplate(brief.projectType);
	const phaseNames = template.defaultPhases;

	return phaseNames.map((name, i) => ({
		id: String(i),
		title: name,
		goal: `Complete ${name.toLowerCase()} phase`,
		days: "",
		description: "",
		tasks: [],
	}));
}

/**
 * Replace template placeholders with actual project values.
 */
function replaceTemplatePlaceholders(text: string, projectName: string): string {
	const projectUnderscore = projectName.toLowerCase().replace(/-/g, "_");
	return text
		.replace(/\{project\}/g, projectUnderscore)
		.replace(/\{project_underscore\}/g, projectUnderscore);
}

/**
 * Get the previous subtask ID in the sequence for prerequisite chaining.
 */
function getPreviousSubtaskId(currentId: string, phases: PhaseTemplate[]): string | null {
	// Collect all subtask IDs in order
	const allIds: string[] = [];
	for (const phase of phases) {
		for (const task of phase.tasks) {
			for (const subtask of task.subtasks) {
				allIds.push(subtask.id);
			}
		}
	}

	const currentIndex = allIds.indexOf(currentId);
	if (currentIndex <= 0) {
		return null; // First subtask has no prerequisite
	}
	return allIds[currentIndex - 1];
}

/**
 * Get a subtask's title by its ID.
 */
function getSubtaskTitle(id: string, phases: PhaseTemplate[], projectName: string): string | null {
	for (const phase of phases) {
		for (const task of phase.tasks) {
			for (const subtask of task.subtasks) {
				if (subtask.id === id) {
					return replaceTemplatePlaceholders(subtask.title, projectName);
				}
			}
		}
	}
	return null;
}

/**
 * Generate detailed DEVELOPMENT_PLAN.md with paint-by-numbers subtasks.
 * Each subtask has explicit deliverables, files, and success criteria.
 */
export function generatePlan(briefContent: string): string {
	const brief = parseBrief(briefContent);
	const techStack = generateTechStack(brief);
	const projectType = (brief.projectType.toLowerCase().replace(/[\s-]/g, "_") || "cli") as
		| "cli"
		| "web_app"
		| "api"
		| "library";

	// Get detailed task templates for this project type
	const phaseTemplates = PROJECT_TYPE_TASKS[projectType] || PROJECT_TYPE_TASKS.cli;

	const techStackSection = Object.entries(techStack)
		.filter(([key, value]) => value && key !== "additionalTools")
		.map(([key, value]) => `- **${formatKey(key)}**: ${value}`)
		.join("\n");

	// Build progress tracking section
	const progressSection = phaseTemplates
		.map((phase) => {
			const taskLines = phase.tasks
				.flatMap((task) =>
					task.subtasks.map(
						(subtask) => `- [ ] ${subtask.id}: ${replaceTemplatePlaceholders(subtask.title, brief.projectName)}`
					)
				)
				.join("\n");
			return `### Phase ${phase.id}: ${phase.title}\n${taskLines}`;
		})
		.join("\n\n");

	// Build detailed phase sections with full subtask details
	const phasesSection = phaseTemplates
		.map((phase) => {
			const tasksSection = phase.tasks
				.map((task) => {
					const subtasksSection = task.subtasks
						.map((subtask) => {
							const deliverables = subtask.deliverables
								.map((d) => `- [ ] ${replaceTemplatePlaceholders(d, brief.projectName)}`)
								.join("\n");
							const filesToCreate =
								subtask.filesToCreate.length > 0
									? subtask.filesToCreate
											.map((f) => `- \`${replaceTemplatePlaceholders(f, brief.projectName)}\``)
											.join("\n")
									: "- None";
							const filesToModify =
								subtask.filesToModify.length > 0
									? subtask.filesToModify
											.map((f) => `- \`${replaceTemplatePlaceholders(f, brief.projectName)}\``)
											.join("\n")
									: "- None";
							const successCriteria = subtask.successCriteria
								.map((c) => `- [ ] ${replaceTemplatePlaceholders(c, brief.projectName)}`)
								.join("\n");
							// Technology Decisions are mandatory - provide default if not specified
							const techDecisionsContent = subtask.techDecisions && subtask.techDecisions.length > 0
								? subtask.techDecisions.map((t) => `- ${replaceTemplatePlaceholders(t, brief.projectName)}`).join("\n")
								: `- Follow project conventions established in Phase 0\n- Maintain consistency with existing codebase patterns`;
							const techDecisions = `\n**Technology Decisions**:\n${techDecisionsContent}\n`;

							// Track prerequisite for this subtask (marked [x] as it must be complete before starting)
							const prereqId = getPreviousSubtaskId(subtask.id, phaseTemplates);
							const prereqTitle = prereqId ? getSubtaskTitle(prereqId, phaseTemplates, brief.projectName) : null;
							const prerequisite = prereqId
								? `- [x] ${prereqId}: ${prereqTitle}`
								: "- None (first subtask)";

							return `**Subtask ${subtask.id}: ${replaceTemplatePlaceholders(subtask.title, brief.projectName)} (Single Session)**

**Prerequisites**:
${prerequisite}

**Deliverables**:
${deliverables}
${techDecisions}
**Files to Create**:
${filesToCreate}

**Files to Modify**:
${filesToModify}

**Success Criteria**:
${successCriteria}

---

**Completion Notes**:
- **Implementation**: (describe what was done)
- **Files Created**:
  - (filename) - (line count) lines
- **Files Modified**:
  - (filename)
- **Tests**: (X tests, Y% coverage)
- **Build**: (ruff: pass/fail, mypy: pass/fail)
- **Branch**: feature/${task.id}-${subtask.title.toLowerCase().replace(/[^a-z0-9]+/g, "-").slice(0, 20)}
- **Notes**: (any additional context)

---`;
						})
						.join("\n\n");

					return `### Task ${task.id}: ${task.title}

**Git**: Create branch \`feature/${task.id}-${task.title.toLowerCase().replace(/[^a-z0-9]+/g, "-").slice(0, 20)}\` when starting first subtask. Commit after each subtask. Squash merge to main when task complete.

${subtasksSection}`;
				})
				.join("\n\n");

			const daysLine = phase.days ? `**Duration**: ${phase.days}\n\n` : "";

			return `## Phase ${phase.id}: ${phase.title}

**Goal**: ${phase.goal}
${daysLine}
${tasksSection}`;
		})
		.join("\n\n---\n\n");

	// Add feature-specific phases based on the project's MVP features
	const featurePhasesSection = generateFeaturePhases(brief);

	// Add deferred phases for nice-to-have features (Phase X.5 v2)
	const deferredPhasesSection = generateDeferredPhases(brief);

	return `# ${brief.projectName} - Development Plan

## How to Use This Plan

**For Claude Code**: Read this plan, find the subtask ID from the prompt, complete ALL checkboxes, update completion notes, commit.

**For You**: Use this prompt (change only the subtask ID):
\`\`\`
please re-read claude.md and DEVELOPMENT_PLAN.md (the entire documents, for context), then continue with [X.Y.Z], following all of the development plan and claude.md rules.
\`\`\`

---

## Project Overview

**Project Name**: ${brief.projectName}
**Goal**: ${brief.primaryGoal}
**Target Users**: ${brief.targetUsers}
**Timeline**: ${brief.timeline || "Not specified"}

**MVP Scope**:
${brief.keyFeatures.map((f) => `- [ ] ${f}`).join("\n")}

---

## Technology Stack

${techStackSection}

---

## Progress Tracking

${progressSection}

**Current**: Phase 0
**Next**: 0.1.1

---

${phasesSection}

${featurePhasesSection}

${deferredPhasesSection}

## Git Workflow

### Branch Strategy
- **ONE branch per TASK** (e.g., \`feature/1-2-${brief.projectName.toLowerCase()}-core\`)
- **NO branches for individual subtasks** - subtasks are commits within the task branch
- Create branch when starting first subtask of a task
- Branch naming: \`feature/{phase}-{task}-{description}\`

### Commit Strategy
- **One commit per subtask** with semantic message
- Format: \`feat(scope): description\` or \`fix(scope): description\`
- Types: \`feat\`, \`fix\`, \`refactor\`, \`test\`, \`docs\`, \`chore\`
- Example: \`feat(parser): implement markdown parsing with GFM support\`

### Merge Strategy
- **Squash merge when task is complete** (all subtasks done)
- PR required for production branches
- Delete feature branch after merge

### Workflow Example
\`\`\`bash
# Starting Task 1.2 (first subtask is 1.2.1)
git checkout -b feature/1-2-user-auth

# After completing subtask 1.2.1
git add . && git commit -m "feat(auth): implement user model"

# After completing subtask 1.2.2
git add . && git commit -m "feat(auth): add password hashing"

# After completing subtask 1.2.3 (task complete)
git add . && git commit -m "feat(auth): add login endpoint"
git push -u origin feature/1-2-user-auth
# Create PR, squash merge to main, delete branch
\`\`\`

---

## Ready to Build

You now have a development plan so detailed that even Claude with Haiku can implement it. Each subtask is paint-by-numbers: explicit deliverables, specific files, and testable success criteria.

**To start implementation**, use this prompt (change only the subtask ID):

\`\`\`
Please read CLAUDE.md and DEVELOPMENT_PLAN.md completely, then implement subtask [0.1.1], following all rules and marking checkboxes as you complete each item.
\`\`\`

**Pro tip**: Start with 0.1.1 and work through subtasks in order. Each one builds on the previous.

---

*Generated by DevPlan MCP Server*
`;
}

/**
 * Analyze a feature string and categorize it for detailed template generation.
 */
function categorizeFeature(feature: string): {
	category:
		| "file_io"
		| "parsing"
		| "pdf_generation"
		| "data_transform"
		| "api_endpoint"
		| "ui_component"
		| "config"
		| "generic";
	keywords: string[];
} {
	const lowerFeature = feature.toLowerCase();
	const keywords: string[] = [];

	// Extract file types for context
	const fileTypeMatch = lowerFeature.match(
		/\b(markdown|md|pdf|json|yaml|csv|xml|html|txt|file|document)\b/g
	);
	if (fileTypeMatch) keywords.push(...fileTypeMatch);

	// PDF/document generation patterns (check before generic "generate")
	if (
		(lowerFeature.includes("generate") || lowerFeature.includes("create") || lowerFeature.includes("output")) &&
		(lowerFeature.includes("pdf") || lowerFeature.includes("document"))
	) {
		return { category: "pdf_generation", keywords };
	}

	// Parsing patterns (check before generic file_io)
	if (
		lowerFeature.includes("parse") ||
		(lowerFeature.includes("read") && (lowerFeature.includes("markdown") || lowerFeature.includes("md")))
	) {
		return { category: "parsing", keywords };
	}

	// File I/O patterns - conversion
	if (
		lowerFeature.includes("convert") ||
		lowerFeature.includes("export") ||
		lowerFeature.includes("import")
	) {
		return { category: "file_io", keywords };
	}

	// Batch/watch patterns
	if (
		lowerFeature.includes("batch") ||
		lowerFeature.includes("multiple") ||
		lowerFeature.includes("watch") ||
		lowerFeature.includes("monitor")
	) {
		return { category: "file_io", keywords: ["batch", ...keywords] };
	}

	// Data transformation patterns
	if (
		lowerFeature.includes("transform") ||
		lowerFeature.includes("process") ||
		lowerFeature.includes("highlight") ||
		lowerFeature.includes("format") ||
		lowerFeature.includes("style") ||
		lowerFeature.includes("theme") ||
		lowerFeature.includes("table of contents") ||
		lowerFeature.includes("toc")
	) {
		return { category: "data_transform", keywords };
	}

	// Config/options patterns
	if (
		lowerFeature.includes("custom") ||
		lowerFeature.includes("config") ||
		lowerFeature.includes("option") ||
		lowerFeature.includes("setting") ||
		lowerFeature.includes("output path") ||
		lowerFeature.includes("filename")
	) {
		return { category: "config", keywords };
	}

	// Generic file operations
	if (
		lowerFeature.includes("read") ||
		lowerFeature.includes("write") ||
		lowerFeature.includes("load") ||
		lowerFeature.includes("save")
	) {
		return { category: "file_io", keywords };
	}

	return { category: "generic", keywords };
}

/**
 * Generate detailed implementation guidance based on feature category.
 */
function generateFeatureImplementation(
	feature: string,
	projectName: string,
	phaseNum: number,
	techStack: string[]
): {
	deliverables: string[];
	filesToCreate: string[];
	filesToModify: string[];
	successCriteria: string[];
	techDecisions?: string[];
	codeGuidance: string;
} {
	const { category, keywords } = categorizeFeature(feature);
	const projectUnderscore = projectName.toLowerCase().replace(/-/g, "_");

	switch (category) {
		case "parsing": {
			const isMarkdown = keywords.includes("markdown") || keywords.includes("md");
			const isJson = keywords.includes("json");
			const isYaml = keywords.includes("yaml");

			if (isMarkdown) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/parser.py\` - Markdown parsing module`,
						`Implement \`parse_markdown_file(path: Path) -> str\` - Read file and parse to HTML`,
						`Implement \`parse_markdown_content(content: str) -> str\` - Parse string to HTML`,
						`Implement \`create_parser(code_highlighter=None) -> MarkdownIt\` - Configure parser instance`,
						`Implement \`CustomRenderer\` class - Custom renderer with code block hooks`,
						`Handle GFM extensions: tables, task lists (\`- [ ]\`), strikethrough (\`~~text~~\`)`,
						`Add \`markdown-it-py[plugins]\` to pyproject.toml dependencies`,
						`Create unit tests achieving >80% coverage`,
						`Create test fixtures with sample markdown files`,
					],
					filesToCreate: [
						`${projectUnderscore}/parser.py`,
						`tests/test_parser.py`,
						`tests/fixtures/sample.md`,
						`tests/fixtures/complex.md`,
					],
					filesToModify: [`pyproject.toml`],
					successCriteria: [
						`\`parse_markdown_file()\` reads file with UTF-8 encoding and returns HTML string`,
						`\`parse_markdown_content()\` handles all CommonMark elements (headers h1-h6, lists, links, images, code blocks)`,
						`GFM tables render as HTML \`<table>\` elements`,
						`Task lists (\`- [ ]\` / \`- [x]\`) render as checkboxes`,
						`Code blocks with language hints preserve language for highlighter: \`\`\`python -> \`<code class="language-python">\``,
						`Empty input returns empty string (no exceptions)`,
						`File not found raises \`FileNotFoundError\` with descriptive message`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Use markdown-it-py for CommonMark + GFM parsing (not mistune - less GFM support)`,
						`Custom renderer class for code block hooks (enables syntax highlighting integration)`,
						`Return HTML string (not DOM) - simpler for PDF generation pipeline`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/parser.py
from markdown_it import MarkdownIt
from markdown_it.renderer import RendererHTML

class CustomRenderer(RendererHTML):
    """Custom renderer with hooks for code block processing."""

    def __init__(self, code_highlighter=None):
        super().__init__()
        self.code_highlighter = code_highlighter

    def fence(self, tokens, idx, options, env):
        """Render fenced code blocks with optional syntax highlighting."""
        token = tokens[idx]
        code = token.content
        lang = token.info.strip() if token.info else None

        if self.code_highlighter and lang:
            return self.code_highlighter(code, lang)

        lang_class = f' class="language-{lang}"' if lang else ''
        escaped = self.escapeHtml(code)
        return f'<pre><code{lang_class}>{escaped}</code></pre>\\n'


def create_parser(code_highlighter=None) -> MarkdownIt:
    """
    Create configured markdown parser.

    Args:
        code_highlighter: Optional function(code, lang) -> html for syntax highlighting

    Returns:
        Configured MarkdownIt instance
    """
    md = MarkdownIt("gfm-like")  # GitHub Flavored Markdown
    md.renderer = CustomRenderer(code_highlighter)
    return md


def parse_markdown(content: str, code_highlighter=None) -> str:
    """
    Parse markdown content to HTML.

    Args:
        content: Markdown text to parse
        code_highlighter: Optional syntax highlighter function

    Returns:
        HTML string
    """
    parser = create_parser(code_highlighter)
    return parser.render(content)
\`\`\`

**Test fixtures** (tests/fixtures/sample.md):
\`\`\`markdown
# Heading 1
## Heading 2

Paragraph with **bold** and *italic* text.

- List item 1
- List item 2
  - Nested item

\\\`\\\`\\\`python
def hello():
    print("Hello, World!")
\\\`\\\`\\\`

| Column 1 | Column 2 |
|----------|----------|
| Cell 1   | Cell 2   |
\`\`\``,
				};
			}

			if (isJson) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/parser.py\` with JSON parsing utilities`,
						`Use built-in \`json\` module with proper error handling`,
						`Support both file paths and string content as input`,
						`Validate JSON structure against expected schema (if applicable)`,
						`Write tests with valid and invalid JSON samples`,
					],
					filesToCreate: [`${projectUnderscore}/parser.py`, `tests/test_parser.py`],
					filesToModify: [],
					successCriteria: [
						`Valid JSON parses without errors`,
						`Invalid JSON raises descriptive error with line number`,
						`Large files (>1MB) parse efficiently`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/parser.py
import json
from pathlib import Path
from typing import Any

class ParseError(Exception):
    """Raised when parsing fails."""
    pass

def parse_json(content: str | Path) -> Any:
    """
    Parse JSON content or file.

    Args:
        content: JSON string or path to JSON file

    Returns:
        Parsed Python object

    Raises:
        ParseError: If JSON is invalid
    """
    if isinstance(content, Path) or (isinstance(content, str) and Path(content).exists()):
        path = Path(content)
        content = path.read_text(encoding="utf-8")

    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        raise ParseError(f"Invalid JSON at line {e.lineno}, column {e.colno}: {e.msg}") from e
\`\`\``,
				};
			}

			// Generic parsing
			return {
				deliverables: [
					`Create \`${projectUnderscore}/parser.py\` with parsing logic`,
					`Implement input validation and error handling`,
					`Support both file paths and string content`,
					`Write tests with various input samples`,
				],
				filesToCreate: [`${projectUnderscore}/parser.py`, `tests/test_parser.py`],
				filesToModify: [],
				successCriteria: [
					`Valid input parses correctly`,
					`Invalid input raises descriptive errors`,
					`Edge cases (empty, very large) handled gracefully`,
				],
				codeGuidance: "",
			};
		}

		case "pdf_generation": {
			return {
				deliverables: [
					`Create \`${projectUnderscore}/pdf.py\` - PDF generation module`,
					`Implement \`generate_pdf(html: str, output_path: Path, css: str = None) -> Path\` - Core PDF generation`,
					`Implement \`convert_file(input_path: Path, output_path: Path = None) -> Path\` - File-to-PDF conversion`,
					`Implement \`get_default_css() -> str\` - Return default print stylesheet`,
					`Create \`${projectUnderscore}/templates/base.html\` - HTML wrapper template with {css} and {content} placeholders`,
					`Define \`DEFAULT_CSS\` constant with @page rules, typography, code block styles`,
					`Add \`weasyprint>=60.0\` to pyproject.toml dependencies`,
					`Add \`convert\` command to CLI: \`${projectName} convert <input> [-o output]\``,
					`Create unit tests achieving >80% coverage`,
				],
				filesToCreate: [
					`${projectUnderscore}/pdf.py`,
					`${projectUnderscore}/templates/base.html`,
					`tests/test_pdf.py`,
				],
				filesToModify: [`${projectUnderscore}/cli.py`, `pyproject.toml`],
				successCriteria: [
					`\`generate_pdf()\` creates valid PDF file at specified path`,
					`\`convert_file()\` defaults output to input path with .pdf extension`,
					`PDF uses A4 page size with 2cm margins (@page CSS rule)`,
					`Default CSS includes readable typography (11pt body, proper heading sizes)`,
					`Code blocks use monospace font with light background`,
					`Output directory is created if it doesn't exist (mkdir parents=True)`,
					`ImportError for WeasyPrint shows message: "Install WeasyPrint: pip install weasyprint"`,
					`All tests pass with >80% coverage`,
				],
				techDecisions: [
					`Use WeasyPrint for HTML-to-PDF (best CSS support, active development)`,
					`A4 page size as default (international standard, fits US Letter content)`,
					`2cm margins (balanced whitespace without wasting paper)`,
					`Template pattern with {css} and {content} placeholders (flexible, testable)`,
				],
				codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/pdf.py
from pathlib import Path
from weasyprint import HTML, CSS
from weasyprint.text.fonts import FontConfiguration

# Base HTML template
BASE_TEMPLATE = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <style>{css}</style>
</head>
<body>
{content}
</body>
</html>"""

# Default print styles
DEFAULT_CSS = """
@page {
    size: A4;
    margin: 2cm;
}
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    font-size: 11pt;
    line-height: 1.5;
}
h1 { font-size: 24pt; margin-top: 0; }
h2 { font-size: 18pt; }
h3 { font-size: 14pt; }
code {
    font-family: "SF Mono", Monaco, "Cascadia Code", monospace;
    font-size: 10pt;
    background: #f5f5f5;
    padding: 0.2em 0.4em;
}
pre {
    background: #f5f5f5;
    padding: 1em;
    overflow-x: auto;
    border-radius: 4px;
}
"""


def generate_pdf(
    html_content: str,
    output_path: str | Path,
    css: str | None = None,
) -> Path:
    """
    Generate PDF from HTML content.

    Args:
        html_content: HTML string to convert
        output_path: Where to save the PDF
        css: Optional custom CSS (uses default if not provided)

    Returns:
        Path to generated PDF file
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    full_html = BASE_TEMPLATE.format(
        css=css or DEFAULT_CSS,
        content=html_content
    )

    font_config = FontConfiguration()
    html = HTML(string=full_html)
    html.write_pdf(output_path, font_config=font_config)

    return output_path


def convert_markdown_to_pdf(
    markdown_path: str | Path,
    output_path: str | Path | None = None,
    css: str | None = None,
) -> Path:
    """
    Convert markdown file to PDF.

    Args:
        markdown_path: Path to markdown file
        output_path: Output PDF path (defaults to same name with .pdf)
        css: Optional custom CSS

    Returns:
        Path to generated PDF
    """
    from ${projectUnderscore}.parser import parse_markdown

    markdown_path = Path(markdown_path)
    if output_path is None:
        output_path = markdown_path.with_suffix(".pdf")

    content = markdown_path.read_text(encoding="utf-8")
    html = parse_markdown(content)

    return generate_pdf(html, output_path, css)
\`\`\`

**Base template** (${projectUnderscore}/templates/base.html):
\`\`\`html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>{{ css }}</style>
</head>
<body>
{{ content }}
</body>
</html>
\`\`\`

**Dependencies** (add to pyproject.toml):
\`\`\`toml
dependencies = [
    "weasyprint>=60.0",
    "markdown-it-py>=3.0",
]
\`\`\``,
			};
		}

		case "file_io": {
			const isConversion =
				feature.toLowerCase().includes("convert") || feature.toLowerCase().includes("export");
			const isBatch =
				feature.toLowerCase().includes("batch") || feature.toLowerCase().includes("multiple");
			const isWatch = feature.toLowerCase().includes("watch");

			if (isWatch) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/watcher.py\` - File watching module`,
						`Implement \`WatchHandler\` class extending FileSystemEventHandler`,
						`Implement \`watch_file(path: Path, on_change: Callable) -> None\` - Watch single file`,
						`Implement debouncing in WatchHandler: ignore events within 300ms of last event`,
						`Add \`--watch\` / \`-w\` flag to convert command`,
						`Handle graceful shutdown: catch KeyboardInterrupt, stop observer, print "Stopped watching"`,
						`Add \`watchdog>=3.0\` to pyproject.toml dependencies`,
						`Create unit tests achieving >80% coverage`,
					],
					filesToCreate: [`${projectUnderscore}/watcher.py`, `tests/test_watcher.py`],
					filesToModify: [`${projectUnderscore}/cli.py`, `pyproject.toml`],
					successCriteria: [
						`\`WatchHandler.on_modified()\` calls callback only if >300ms since last call`,
						`\`watch_file()\` prints "Watching {path} for changes... (Ctrl+C to stop)"`,
						`File modification triggers rebuild within 1 second`,
						`Multiple rapid saves (within 300ms) trigger only one rebuild`,
						`Ctrl+C prints "Stopped watching" and exits with code 0`,
						`Watching non-existent file raises FileNotFoundError`,
						`File deletion while watching prints warning, continues watching parent directory`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Use watchdog library for cross-platform file system events`,
						`300ms debounce delay (balances responsiveness vs duplicate events)`,
						`Watch parent directory, filter for target file (watchdog limitation)`,
						`Use time.time() for debounce tracking (simple, no external deps)`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/watcher.py
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time

class ConversionHandler(FileSystemEventHandler):
    def __init__(self, convert_fn, debounce_ms=300):
        self.convert_fn = convert_fn
        self.debounce_ms = debounce_ms
        self._last_event = 0

    def on_modified(self, event):
        if event.is_directory:
            return
        # Debounce rapid events
        now = time.time() * 1000
        if now - self._last_event < self.debounce_ms:
            return
        self._last_event = now
        self.convert_fn(event.src_path)

def watch_file(path: str, convert_fn, on_change_msg: str = "Rebuilt"):
    """Watch a file and call convert_fn when it changes."""
    handler = ConversionHandler(convert_fn)
    observer = Observer()
    observer.schedule(handler, path=str(Path(path).parent), recursive=False)
    observer.start()
    print(f"Watching {path} for changes... (Ctrl+C to stop)")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()
\`\`\``,
				};
			}

			if (isBatch) {
				return {
					deliverables: [
						`Implement \`expand_inputs(patterns: list[str]) -> list[Path]\` - Expand globs and directories to file list`,
						`Implement \`convert_batch(files: list[Path], output_dir: Path = None) -> BatchResult\` - Convert multiple files`,
						`Implement \`BatchResult\` dataclass with fields: succeeded (list), failed (list), total_time (float)`,
						`Update CLI to accept multiple positional arguments: \`${projectName} convert <files>...\``,
						`Add \`--output-dir\` / \`-d\` option for batch output location`,
						`Add progress indicator using click.progressbar showing "[1/5] Converting file.md..."`,
						`Handle partial failures: continue processing, report failures at end`,
						`Create unit tests achieving >80% coverage`,
					],
					filesToCreate: [`tests/test_batch.py`],
					filesToModify: [`${projectUnderscore}/cli.py`, `${projectUnderscore}/pdf.py`],
					successCriteria: [
						`\`expand_inputs(["*.md"])\` returns list of matching Path objects`,
						`\`expand_inputs(["./docs/"])\` returns all .md files in directory recursively`,
						`\`convert_batch()\` returns BatchResult with counts of succeeded/failed`,
						`Progress bar shows current file name and count: "[1/5] Converting README.md"`,
						`\`--output-dir ./pdfs/\` creates directory if needed and places outputs there`,
						`If 2 of 5 files fail, exit code is non-zero and stderr lists failed files`,
						`Keyboard interrupt (Ctrl+C) stops gracefully, reports partial progress`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Use glob.glob() with recursive=True for pattern expansion`,
						`Use click.progressbar for progress display (built into Click)`,
						`Continue on failure, collect errors, report at end (don't fail fast)`,
						`Exit code = number of failed files (0 = success, >0 = some failures)`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# In cli.py
@click.argument("inputs", nargs=-1, required=True)
@click.option("--output-dir", "-d", type=click.Path(), help="Output directory for batch")
@click.option("--jobs", "-j", default=1, help="Parallel jobs")
def convert(inputs: tuple[str, ...], output_dir: str | None, jobs: int):
    # Expand globs
    files = []
    for pattern in inputs:
        if "*" in pattern:
            files.extend(glob.glob(pattern, recursive=True))
        elif Path(pattern).is_dir():
            files.extend(Path(pattern).glob("**/*.md"))
        else:
            files.append(pattern)

    # Process with progress
    with click.progressbar(files, label="Converting") as bar:
        for f in bar:
            output = determine_output_path(f, output_dir)
            convert_file(f, output)
\`\`\``,
				};
			}

			if (isConversion) {
				// Extract source and target formats from feature
				const formats = keywords.filter((k) =>
					["markdown", "md", "pdf", "json", "yaml", "csv", "html"].includes(k)
				);
				const sourceFormat = formats[0] || "input";
				const targetFormat = formats[1] || "output";

				return {
					deliverables: [
						`Create \`${projectUnderscore}/converter.py\` with \`convert(input_path, output_path)\` function`,
						`Parse ${sourceFormat} input using appropriate library`,
						`Generate ${targetFormat} output with proper formatting`,
						`Add \`convert\` command to CLI with input/output arguments`,
						`Handle encoding (UTF-8) and file not found errors`,
						`Write unit tests with sample fixtures`,
					],
					filesToCreate: [
						`${projectUnderscore}/converter.py`,
						`tests/test_converter.py`,
						`tests/fixtures/sample.md`,
					],
					filesToModify: [`${projectUnderscore}/cli.py`],
					successCriteria: [
						`\`${projectName} convert input.md\` produces \`input.pdf\` in same directory`,
						`\`${projectName} convert input.md -o custom.pdf\` works`,
						`Informative error message for missing input file`,
						`Tests cover: valid conversion, missing file, invalid format`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/converter.py
from pathlib import Path

def convert(input_path: str | Path, output_path: str | Path | None = None) -> Path:
    """
    Convert ${sourceFormat} file to ${targetFormat}.

    Args:
        input_path: Path to input ${sourceFormat} file
        output_path: Optional output path. Defaults to input with .${targetFormat} extension.

    Returns:
        Path to generated ${targetFormat} file

    Raises:
        FileNotFoundError: If input file doesn't exist
        ConversionError: If conversion fails
    """
    input_path = Path(input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    if output_path is None:
        output_path = input_path.with_suffix(".${targetFormat}")
    else:
        output_path = Path(output_path)

    # Read input
    content = input_path.read_text(encoding="utf-8")

    # Transform (implement conversion logic here)
    result = transform_content(content)

    # Write output
    output_path.write_bytes(result)  # or write_text for text formats

    return output_path
\`\`\``,
				};
			}

			// Generic file I/O
			return {
				deliverables: [
					`Implement file reading with proper encoding handling (UTF-8)`,
					`Implement file writing with atomic writes (write to temp, then rename)`,
					`Add CLI options for input/output paths`,
					`Handle file permissions and disk space errors`,
					`Write tests with temporary files`,
				],
				filesToCreate: [`${projectUnderscore}/io.py`, `tests/test_io.py`],
				filesToModify: [`${projectUnderscore}/cli.py`],
				successCriteria: [
					`Can read files with various encodings`,
					`Atomic writes prevent partial output on error`,
					`Clear error messages for permission denied, disk full`,
				],
				codeGuidance: "",
			};
		}

		case "data_transform": {
			const isHighlight = feature.toLowerCase().includes("highlight");
			const isTheme =
				feature.toLowerCase().includes("theme") ||
				feature.toLowerCase().includes("style") ||
				feature.toLowerCase().includes("css");
			const isToc =
				feature.toLowerCase().includes("table of contents") || feature.toLowerCase().includes("toc");

			if (isHighlight) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/highlighter.py\` - Syntax highlighting module`,
						`Implement \`highlight_code(code: str, language: str = None) -> str\` - Apply syntax highlighting`,
						`Implement \`get_lexer(language: str = None, code: str = None) -> Lexer\` - Get appropriate Pygments lexer`,
						`Implement \`get_highlight_css(theme: str = "default") -> str\` - Get CSS for highlight theme`,
						`Implement \`SUPPORTED_THEMES: list[str]\` - List of available Pygments themes`,
						`Integrate with parser: pass \`highlight_code\` as \`code_highlighter\` to \`create_parser()\``,
						`Add \`--highlight-theme <name>\` CLI option (default: "default")`,
						`Add \`pygments>=2.0\` to pyproject.toml dependencies`,
						`Create unit tests achieving >80% coverage`,
					],
					filesToCreate: [
						`${projectUnderscore}/highlighter.py`,
						`tests/test_highlighter.py`,
					],
					filesToModify: [`${projectUnderscore}/parser.py`, `${projectUnderscore}/cli.py`, `pyproject.toml`],
					successCriteria: [
						`\`highlight_code()\` returns HTML with \`<span class="...">\` syntax highlighting spans`,
						`\`get_lexer()\` returns correct lexer for "python", "javascript", "bash", "json"`,
						`\`get_lexer()\` with unknown language falls back to TextLexer (plain text)`,
						`\`get_lexer(None, code)\` auto-detects language from code content`,
						`\`get_highlight_css("monokai")\` returns valid CSS for monokai theme`,
						`Integration: markdown code blocks are highlighted in final PDF output`,
						`\`--highlight-theme invalid\` shows error with list of available themes`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Use Pygments for syntax highlighting (industry standard, 500+ languages)`,
						`HtmlFormatter with cssclass="highlight" for consistent CSS targeting`,
						`Fallback to TextLexer for unknown languages (never fail, just don't highlight)`,
						`Auto-detection as fallback only (explicit language preferred for accuracy)`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/highlighter.py
from pygments import highlight
from pygments.lexers import get_lexer_by_name, guess_lexer, TextLexer
from pygments.formatters import HtmlFormatter

def highlight_code(code: str, language: str | None = None) -> str:
    """
    Apply syntax highlighting to code block.

    Args:
        code: Source code to highlight
        language: Language identifier (python, js, etc.) or None for auto-detect

    Returns:
        HTML string with syntax highlighting spans
    """
    try:
        if language:
            lexer = get_lexer_by_name(language, stripall=True)
        else:
            lexer = guess_lexer(code)
    except Exception:
        lexer = TextLexer()

    formatter = HtmlFormatter(nowrap=True, cssclass="highlight")
    return highlight(code, lexer, formatter)

def get_highlight_css(theme: str = "default") -> str:
    """Get CSS for syntax highlighting theme."""
    formatter = HtmlFormatter(style=theme)
    return formatter.get_style_defs(".highlight")
\`\`\``,
				};
			}

			if (isTheme) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/themes/__init__.py\` - Theme management module`,
						`Implement \`get_theme_css(theme: str) -> str\` - Load built-in theme CSS`,
						`Implement \`load_custom_css(path: Path) -> str\` - Load user CSS file`,
						`Implement \`list_themes() -> list[str]\` - Return available theme names`,
						`Define \`BUILT_IN_THEMES = ["default", "minimal"]\` constant`,
						`Create \`${projectUnderscore}/themes/default.css\` - Professional theme with system fonts`,
						`Create \`${projectUnderscore}/themes/minimal.css\` - Clean, minimal styling`,
						`Add \`--theme <name>\` CLI option (default: "default")`,
						`Add \`--css <path>\` CLI option for custom CSS file`,
						`Create unit tests achieving >80% coverage`,
					],
					filesToCreate: [
						`${projectUnderscore}/themes/__init__.py`,
						`${projectUnderscore}/themes/default.css`,
						`${projectUnderscore}/themes/minimal.css`,
						`tests/test_themes.py`,
					],
					filesToModify: [`${projectUnderscore}/pdf.py`, `${projectUnderscore}/cli.py`],
					successCriteria: [
						`\`get_theme_css("default")\` returns CSS string with body, heading, code styles`,
						`\`get_theme_css("minimal")\` returns different CSS with simpler styles`,
						`\`get_theme_css("invalid")\` raises ValueError listing available themes`,
						`\`load_custom_css()\` reads file and returns CSS string`,
						`\`load_custom_css()\` raises FileNotFoundError for missing files`,
						`\`list_themes()\` returns ["default", "minimal"]`,
						`\`--theme minimal\` CLI option applies minimal.css to PDF output`,
						`\`--css custom.css\` loads and applies user CSS, overriding default`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Store themes as .css files in package (not embedded strings) - easier to edit`,
						`Use importlib.resources for package data access (Python 3.9+ standard)`,
						`--css overrides --theme completely (no merging) - simpler mental model`,
						`System font stack for default theme (no web font dependencies)`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/themes/__init__.py
from pathlib import Path

THEMES_DIR = Path(__file__).parent
BUILT_IN_THEMES = ["default", "minimal"]

def get_theme_css(theme: str) -> str:
    """Load CSS for a built-in theme."""
    if theme not in BUILT_IN_THEMES:
        raise ValueError(f"Unknown theme: {theme}. Available: {BUILT_IN_THEMES}")
    css_path = THEMES_DIR / f"{theme}.css"
    return css_path.read_text()

def load_custom_css(path: str | Path) -> str:
    """Load CSS from a custom file."""
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"CSS file not found: {path}")
    return path.read_text()
\`\`\`

**default.css structure**:
\`\`\`css
/* ${projectUnderscore}/themes/default.css */
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 2rem; }
h1, h2, h3 { margin-top: 1.5em; }
code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
pre { background: #f4f4f4; padding: 1em; overflow-x: auto; }
\`\`\``,
				};
			}

			if (isToc) {
				return {
					deliverables: [
						`Create \`${projectUnderscore}/toc.py\` - Table of contents module`,
						`Implement \`TocEntry\` dataclass with fields: level (int), text (str), anchor (str)`,
						`Implement \`extract_headings(html: str, max_depth: int = 6) -> list[TocEntry]\` - Parse headings from HTML`,
						`Implement \`generate_anchor(text: str) -> str\` - Create URL-safe anchor from heading text`,
						`Implement \`add_heading_ids(html: str) -> str\` - Add id attributes to headings`,
						`Implement \`generate_toc_html(entries: list[TocEntry]) -> str\` - Render TOC as HTML nav element`,
						`Add \`--toc\` CLI flag to enable TOC generation`,
						`Add \`--toc-depth N\` CLI option to limit heading levels (default: 6)`,
						`Create unit tests achieving >80% coverage`,
					],
					filesToCreate: [`${projectUnderscore}/toc.py`, `tests/test_toc.py`],
					filesToModify: [`${projectUnderscore}/pdf.py`, `${projectUnderscore}/cli.py`],
					successCriteria: [
						`\`extract_headings()\` finds all h1-h6 tags with their text content`,
						`\`extract_headings(html, max_depth=2)\` only returns h1 and h2 entries`,
						`\`generate_anchor("Hello World!")\` returns "hello-world" (lowercase, hyphenated)`,
						`\`add_heading_ids()\` adds id="anchor" to each heading tag`,
						`\`generate_toc_html()\` returns \`<nav class="toc">...<ul>...<li>...\` structure`,
						`TOC links (\`<a href="#anchor">\`) navigate to correct headings in PDF`,
						`Nested headings (h2 under h1) render as nested \`<ul>\` lists`,
						`Empty document (no headings) returns empty string (no error)`,
						`All tests pass with >80% coverage`,
					],
					techDecisions: [
						`Use regex to extract headings (simpler than HTML parser for this use case)`,
						`Anchor format: lowercase, spaces to hyphens, strip special chars`,
						`TOC as \`<nav class="toc">\` element (semantic HTML, easy to style)`,
						`Insert TOC at beginning of content (before first heading)`,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# ${projectUnderscore}/toc.py
import re
from dataclasses import dataclass

@dataclass
class TocEntry:
    level: int  # 1-6
    text: str
    anchor: str

def extract_headings(html: str, max_depth: int = 6) -> list[TocEntry]:
    """Extract headings from HTML content."""
    pattern = r"<h([1-6])[^>]*id=[\"']([^\"']+)[\"'][^>]*>(.+?)</h\\1>"
    entries = []
    for match in re.finditer(pattern, html, re.IGNORECASE):
        level = int(match.group(1))
        if level <= max_depth:
            entries.append(TocEntry(
                level=level,
                anchor=match.group(2),
                text=re.sub(r"<[^>]+>", "", match.group(3))  # Strip inner tags
            ))
    return entries

def generate_toc_html(entries: list[TocEntry]) -> str:
    """Generate HTML table of contents."""
    if not entries:
        return ""
    lines = ['<nav class="toc"><h2>Table of Contents</h2><ul>']
    for entry in entries:
        indent = "  " * (entry.level - 1)
        lines.append(f'{indent}<li><a href="#{entry.anchor}">{entry.text}</a></li>')
    lines.append("</ul></nav>")
    return "\\n".join(lines)
\`\`\``,
				};
			}

			// Generic transform
			return {
				deliverables: [
					`Design transformation pipeline with clear input/output types`,
					`Implement core transformation function`,
					`Add validation for input data`,
					`Handle edge cases (empty input, malformed data)`,
					`Write tests with various input scenarios`,
				],
				filesToCreate: [`${projectUnderscore}/transform.py`, `tests/test_transform.py`],
				filesToModify: [],
				successCriteria: [
					`Transformation produces expected output for valid input`,
					`Clear error messages for invalid input`,
					`Edge cases handled gracefully`,
				],
				codeGuidance: "",
			};
		}

		case "config": {
			const isOutputPath =
				feature.toLowerCase().includes("output") || feature.toLowerCase().includes("path");

			if (isOutputPath) {
				return {
					deliverables: [
						`Add \`-o\` / \`--output\` option for custom output file path`,
						`Add \`--output-dir\` option for batch operations`,
						`Support filename templates: \`{name}\`, \`{date}\`, \`{ext}\``,
						`Create output directory if it doesn't exist`,
						`Handle path conflicts (overwrite prompt or \`--force\`)`,
						`Write tests for path handling`,
					],
					filesToCreate: [`tests/test_output_paths.py`],
					filesToModify: [`${projectUnderscore}/cli.py`, `${projectUnderscore}/converter.py`],
					successCriteria: [
						`\`-o custom.pdf\` creates file at specified path`,
						`\`--output-dir ./out/\` places output in directory (creates if needed)`,
						`Template \`{name}-converted.pdf\` works correctly`,
						`Refuses to overwrite existing file without \`--force\``,
					],
					codeGuidance: `
**Implementation Pattern**:
\`\`\`python
# In cli.py
@click.option("-o", "--output", type=click.Path(), help="Output file path")
@click.option("--output-dir", type=click.Path(), help="Output directory")
@click.option("--force", is_flag=True, help="Overwrite existing files")
def convert(input_file, output, output_dir, force):
    output_path = determine_output_path(input_file, output, output_dir)

    if output_path.exists() and not force:
        if not click.confirm(f"{output_path} exists. Overwrite?"):
            raise click.Abort()

    # Ensure directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

def determine_output_path(input_path: Path, output: str | None, output_dir: str | None) -> Path:
    if output:
        return Path(output)
    if output_dir:
        return Path(output_dir) / input_path.with_suffix(".pdf").name
    return input_path.with_suffix(".pdf")
\`\`\``,
				};
			}

			// Generic config
			return {
				deliverables: [
					`Define configuration schema with defaults`,
					`Support config file (${projectName}.toml or .${projectName}rc)`,
					`CLI options override config file values`,
					`Add \`--config\` option to specify config file path`,
					`Write tests for config loading and merging`,
				],
				filesToCreate: [`${projectUnderscore}/config.py`, `tests/test_config.py`],
				filesToModify: [`${projectUnderscore}/cli.py`],
				successCriteria: [
					`Config file is auto-discovered in current directory`,
					`CLI options take precedence over config file`,
					`Missing config file uses sensible defaults`,
				],
				codeGuidance: "",
			};
		}

		default: {
			// Generic feature - provide structure but let Claude Code fill details
			return {
				deliverables: [
					`Analyze feature requirements and design API/interface`,
					`Create module \`${projectUnderscore}/<feature_module>.py\``,
					`Implement core functionality with proper error handling`,
					`Add CLI integration (command or options)`,
					`Write comprehensive unit tests (>80% coverage)`,
					`Update README with feature documentation`,
				],
				filesToCreate: [`${projectUnderscore}/<feature_module>.py`, `tests/test_<feature>.py`],
				filesToModify: [`${projectUnderscore}/cli.py`, `README.md`],
				successCriteria: [
					`Feature works as described in PROJECT_BRIEF.md`,
					`All expected inputs produce correct outputs`,
					`Edge cases and errors are handled gracefully`,
					`Tests pass with good coverage`,
				],
				codeGuidance: `
**Note**: This is a custom feature. Before implementing:
1. Read PROJECT_BRIEF.md to understand exact requirements
2. Check existing code patterns in the project
3. Design the interface (function signatures, CLI options)
4. Write tests first (TDD approach)
5. Implement and iterate`,
			};
		}
	}
}

/**
 * Generate a short title for a feature (for prerequisite references).
 */
function getFeatureShortTitle(feature: string): string {
	// Extract key words and limit length
	const words = feature.split(/\s+/).slice(0, 4);
	return words.join(" ");
}

/**
 * Generate additional phases based on the project's specific MVP features.
 * Now with detailed, paint-by-numbers implementation guidance.
 */
function generateFeaturePhases(brief: ProjectBrief): string {
	if (brief.keyFeatures.length === 0) {
		return "";
	}

	// Start feature phases after foundation phases (typically phase 2+)
	const startPhase = 2;
	const techStack = brief.mustUseTech;

	// Track previous subtask for prerequisite chaining
	let previousSubtaskId = "1.1.1";
	let previousSubtaskTitle = "Click CLI Setup";

	const featurePhases = brief.keyFeatures.map((feature, index) => {
		const phaseNum = startPhase + index;
		const featureId = feature
			.toLowerCase()
			.replace(/[^a-z0-9]+/g, "-")
			.slice(0, 30);
		const featureShortTitle = getFeatureShortTitle(feature);

		const impl = generateFeatureImplementation(feature, brief.projectName, phaseNum, techStack);

		const deliverables = impl.deliverables.map((d) => `- [ ] ${d}`).join("\n");
		const filesToCreate =
			impl.filesToCreate.length > 0
				? impl.filesToCreate.map((f) => `- \`${f}\``).join("\n")
				: "- None (modify existing files only)";
		const filesToModify =
			impl.filesToModify.length > 0
				? impl.filesToModify.map((f) => `- \`${f}\``).join("\n")
				: "- None";
		const successCriteria = impl.successCriteria.map((c) => `- [ ] ${c}`).join("\n");
		// Technology Decisions are mandatory - provide default if not specified
		const techDecisionsContent = impl.techDecisions && impl.techDecisions.length > 0
			? impl.techDecisions.map((t) => `- ${t}`).join("\n")
			: `- Follow project conventions established in Phase 0\n- Maintain consistency with existing codebase patterns\n- Prioritize readability and maintainability`;
		const techDecisions = `\n**Technology Decisions**:\n${techDecisionsContent}\n`;
		// Code guidance with implementation examples
		const codeGuidance = impl.codeGuidance ? `\n**Implementation Guide**:\n${impl.codeGuidance}\n` : "";

		// Generate prerequisite reference to previous subtask (marked [x] as it must be complete)
		const prerequisite = `- [x] ${previousSubtaskId}: ${previousSubtaskTitle}`;

		// Update for next iteration
		const currentSubtaskId = `${phaseNum}.1.1`;
		previousSubtaskId = currentSubtaskId;
		previousSubtaskTitle = featureShortTitle;

		return `## Phase ${phaseNum}: ${feature}

**Goal**: Implement ${feature.toLowerCase()}
**Duration**: 1-2 days

### Task ${phaseNum}.1: Core Implementation

**Git**: Create branch \`feature/${phaseNum}-1-${featureId}\` when starting. Commit after subtask. Squash merge to main when task complete.

**Subtask ${phaseNum}.1.1: ${featureShortTitle} (Single Session)**

**Prerequisites**:
${prerequisite}

**Deliverables**:
${deliverables}
${techDecisions}
**Files to Create**:
${filesToCreate}

**Files to Modify**:
${filesToModify}

**Success Criteria**:
${successCriteria}
${codeGuidance}
---

**Completion Notes**:
- **Implementation**: (describe what was done)
- **Files Created**:
  - (filename) - (line count) lines
- **Files Modified**:
  - (filename)
- **Tests**: (X tests, Y% coverage)
- **Build**: (ruff: pass/fail, mypy: pass/fail)
- **Branch**: feature/${phaseNum}-1-${featureId}
- **Notes**: (any additional context)

---`;
	});

	return featurePhases.join("\n\n");
}

/**
 * Generate deferred phases for nice-to-have features (v2).
 * Uses Phase X.5 notation to indicate these are post-MVP.
 */
function generateDeferredPhases(brief: ProjectBrief): string {
	if (!brief.niceToHaveFeatures || brief.niceToHaveFeatures.length === 0) {
		return "";
	}

	// Calculate the starting phase number (after MVP features)
	const lastMvpPhaseNum = 2 + brief.keyFeatures.length - 1;

	const deferredPhases = brief.niceToHaveFeatures.map((feature, index) => {
		// Use X.5 notation to indicate post-MVP
		const phaseNum = `${lastMvpPhaseNum + index + 1}.5`;
		const featureId = feature
			.toLowerCase()
			.replace(/[^a-z0-9]+/g, "-")
			.slice(0, 30);

		return `## Phase ${phaseNum} (v2): ${feature}

**Goal**: Implement ${feature.toLowerCase()} (post-MVP enhancement)
**Duration**: TBD (deferred)
**Status**:  DEFERRED - Implement after MVP completion

### Task ${phaseNum}.1: Implementation (v2)

**Subtask ${phaseNum}.1.1: ${feature} (Deferred)**

**Prerequisites**:
- [ ] All MVP phases complete
- [ ] MVP tested and stable

**Deliverables**:
- [ ] (To be detailed after MVP completion)

**Success Criteria**:
- [ ] Feature works as specified
- [ ] All tests pass
- [ ] Documentation updated

---

**Note**: This phase is deferred until after MVP release. Requirements may be refined based on MVP learnings.

---`;
	});

	if (deferredPhases.length === 0) {
		return "";
	}

	return `## Deferred Phases (v2 Features)

The following phases are planned for v2, after MVP completion. They use Phase X.5 notation to indicate their deferred status.

${deferredPhases.join("\n\n")}`;
}

export function generateClaudeMd(
	briefContent: string,
	language: string = "python",
	testCoverage: number = 80
): string {
	const brief = parseBrief(briefContent);
	const isPython = language === "python";
	const projectUnderscore = brief.projectName.toLowerCase().replace(/-/g, "_");

	return `# CLAUDE.md - Project Rules for ${brief.projectName}

> This document defines HOW Claude Code should work on ${brief.projectName}.
> Read at the start of every session to maintain consistency.

## Core Operating Principles

### 1. Single Session Execution
-  Complete the ENTIRE subtask in this session
-  End every session with a git commit
-  If blocked, document why and mark as BLOCKED

### 2. Read Before Acting
**Every session must begin with:**
1. Read DEVELOPMENT_PLAN.md completely
2. Locate the specific subtask ID from the prompt
3. Verify prerequisites are marked \`[x]\` complete
4. Read completion notes from prerequisites for context

### 3. File Management

**Project Structure:**
\`\`\`
${brief.projectName}/
 ${projectUnderscore}/              # Main package
    __init__.py
    cli.py                   # Click CLI commands
    ...                      # Feature modules
 tests/
    __init__.py
    test_*.py               # Test modules
    fixtures/               # Test data
 pyproject.toml              # Project metadata
 README.md
 CLAUDE.md                   # This file
 DEVELOPMENT_PLAN.md         # Development roadmap
\`\`\`

**Creating Files:**
- Use exact paths specified in subtask
- Add proper module docstrings
- Include type hints on all functions

**Modifying Files:**
- Only modify files listed in subtask
- Preserve existing functionality
- Update related tests

### 4. Testing Requirements

**Unit Tests:**
- Write tests for EVERY new function/class
- Place in \`tests/\` with \`test_\` prefix
- Minimum coverage: ${testCoverage}% overall
- Test success, failure, and edge cases

**Running Tests:**
\`\`\`bash
${isPython ? `# All tests
pytest tests/ -v --cov=${projectUnderscore} --cov-report=term-missing

# Specific test file
pytest tests/test_parser.py -v

# With coverage report
pytest --cov=${projectUnderscore} --cov-report=html` : `# All tests
npm test

# With coverage
npm test -- --coverage`}
\`\`\`

**Before Every Commit:**
- [ ] All tests pass
- [ ] Coverage >${testCoverage}%
- [ ] Linting passes (${isPython ? "ruff" : "eslint"})
- [ ] Type checking passes (${isPython ? "mypy" : "tsc"})

### 5. Completion Protocol

**When a subtask is complete:**

1. **Update DEVELOPMENT_PLAN.md** with completion notes:
\`\`\`markdown
**Completion Notes:**
- **Implementation**: Brief description of what was built
- **Files Created**:
  - \`${projectUnderscore}/parser.py\` (234 lines)
  - \`tests/test_parser.py\` (156 lines)
- **Files Modified**:
  - \`${projectUnderscore}/__init__.py\` (added parser import)
- **Tests**: 12 unit tests (85% coverage)
- **Build**:  Success (all tests pass, linting clean)
- **Branch**: feature/subtask-X-Y-Z
- **Notes**: Any deviations, issues, or future work
\`\`\`

2. **Check all checkboxes** in the subtask (change \`[ ]\` to \`[x]\`)

3. **Git commit** with semantic message:
\`\`\`bash
git add .
git commit -m "feat(parser): Implement markdown parser

- Parse markdown with GFM support
- Extract all headings and code blocks
- Add comprehensive tests
- 85% coverage on parser module"
\`\`\`

4. **Report completion** with summary

### 6. Technology Stack

${isPython ? `**Tech Stack:**
- **Language**: Python 3.11+
- **CLI Framework**: Click 8.1+
- **Testing**: pytest 7.4+, pytest-cov
- **Linting**: ruff 0.1+
- **Type Checking**: mypy 1.7+

**Installing Dependencies:**
\`\`\`bash
pip install -e ".[dev]"  # Editable install with dev dependencies
\`\`\`` : `**Tech Stack:**
- **Language**: TypeScript/Node.js
- **Testing**: Jest or Vitest
- **Linting**: ESLint + Prettier
- **Type Checking**: TypeScript strict mode

**Installing Dependencies:**
\`\`\`bash
npm install
\`\`\``}

### 7. Error Handling

**If you encounter an error:**
1. Attempt to fix using project patterns
2. If blocked, update DEVELOPMENT_PLAN.md:
   \`\`\`markdown
   **Completion Notes:**
   - **Status**:  BLOCKED
   - **Error**: [Detailed error message]
   - **Attempted**: [What was tried]
   - **Root Cause**: [Analysis]
   - **Suggested Fix**: [What should be done]
   \`\`\`
3. Do NOT mark subtask complete if blocked
4. Do NOT commit broken code
5. Report immediately

### 8. Code Quality Standards

${isPython ? `**Python Style:**
- Follow PEP 8
- Type hints on all functions: \`def func(x: int) -> str:\`
- Docstrings: Google style
- Max line length: 100 characters
- Use \`ruff\` for linting
- Use \`mypy\` for type checking

**Example Function:**
\`\`\`python
def parse_brief(brief_path: Path) -> ProjectBrief:
    """Parse PROJECT_BRIEF.md file and extract requirements.

    Args:
        brief_path: Path to PROJECT_BRIEF.md file

    Returns:
        ProjectBrief object with all extracted fields

    Raises:
        FileNotFoundError: If brief file doesn't exist
        ValueError: If brief is malformed or missing required fields

    Example:
        >>> brief = parse_brief(Path("PROJECT_BRIEF.md"))
        >>> brief.project_name
        '${brief.projectName}'
    """
    if not brief_path.exists():
        raise FileNotFoundError(f"Brief file not found: {brief_path}")

    # Implementation...
\`\`\`

**Imports:**
- Standard library first
- Third-party second
- Local imports last
- Alphabetical within each group

**Prohibited:**
- \`print()\` for output (use Click.echo or logging)
- \`exit()\` (raise exceptions instead)
- Bare \`except:\` (catch specific exceptions)
- Global variables (use classes or pass parameters)` : `**TypeScript Style:**
- Use strict mode
- Type all function parameters and returns
- Use interfaces for complex types
- Max line length: 100 characters

**Prohibited:**
- \`console.log()\` in production (use proper logging)
- \`any\` type without explicit justification
- Ignoring Promise rejections`}

### 9. CLI Design Standards

${isPython ? `**Command Structure:**
\`\`\`bash
${brief.projectName.toLowerCase()} <command> [options] [arguments]
\`\`\`

**All commands must:**
- Have \`--help\` text with examples
- Use Click's option validation
- Provide clear error messages
- Support \`--verbose\` for debug output
- Return proper exit codes (0=success, 1=error)

**Example Command:**
\`\`\`python
@click.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('-o', '--output', type=click.Path(), help='Output file path')
@click.option('--verbose', is_flag=True, help='Enable verbose output')
def convert(input_file: str, output: str | None, verbose: bool):
    """Convert markdown file to PDF.

    Example:
        ${brief.projectName.toLowerCase()} convert README.md -o output.pdf
    """
    # Implementation...
\`\`\`` : `**All commands should:**
- Have clear help text
- Validate inputs
- Provide clear error messages
- Return proper exit codes`}

### 10. Build Verification

**Before marking subtask complete:**

\`\`\`bash
${isPython ? `# Linting
ruff check ${projectUnderscore} tests

# Type checking
mypy ${projectUnderscore}

# Tests
pytest tests/ -v --cov=${projectUnderscore} --cov-report=term-missing

# Build package
python -m build

# Install and test CLI
pip install -e .
${brief.projectName.toLowerCase()} --help` : `# Linting
npm run lint

# Type checking
npm run typecheck

# Tests
npm test

# Build
npm run build`}
\`\`\`

**All must pass with no errors.**

## Checklist: Starting a New Session

- [ ] Read DEVELOPMENT_PLAN.md completely
- [ ] Locate subtask ID from prompt
- [ ] Verify prerequisites marked \`[x]\`
- [ ] Read prerequisite completion notes
- [ ] Understand success criteria
- [ ] Ready to code!

## Checklist: Ending a Session

- [ ] All subtask checkboxes checked
- [ ] All tests pass (${isPython ? "pytest" : "npm test"})
- [ ] Linting clean (${isPython ? "ruff" : "eslint"})
- [ ] Type checking clean (${isPython ? "mypy" : "tsc"})
- [ ] Completion notes written
- [ ] Git commit with semantic message
- [ ] User notified

---

**Version**: 1.0
**Last Updated**: ${new Date().toISOString().split("T")[0]}
**Project**: ${brief.projectName}

*Generated by DevPlan MCP Server*
`;
}

function formatKey(key: string): string {
	return key
		.replace(/([A-Z])/g, " $1")
		.replace(/^./, (str) => str.toUpperCase())
		.trim();
}

export function validatePlan(content: string, strict: boolean = false): {
	valid: boolean;
	errors: string[];
	warnings: string[];
	suggestions: string[];
} {
	const errors: string[] = [];
	const warnings: string[] = [];
	const suggestions: string[] = [];

	// Check for required sections
	if (!content.includes("# DEVELOPMENT_PLAN")) {
		errors.push("Missing DEVELOPMENT_PLAN header");
	}

	if (!content.includes("## Technology Stack")) {
		errors.push("Missing Technology Stack section");
	}

	if (!content.includes("## Development Phases")) {
		errors.push("Missing Development Phases section");
	}

	// Check for phases
	const phaseMatches = content.match(/### Phase \d+:/g);
	if (!phaseMatches || phaseMatches.length === 0) {
		errors.push("No phases defined in the plan");
	} else if (phaseMatches.length < 3) {
		warnings.push(`Only ${phaseMatches.length} phases defined, consider adding more structure`);
	}

	// Check for Phase 0 Foundation
	if (!content.includes("Phase 0: Foundation")) {
		warnings.push("Phase 0 should be titled 'Foundation'");
	}

	// Suggestions
	if (!content.includes("Git Workflow")) {
		suggestions.push("Consider adding a Git Workflow section");
	}

	if (!content.includes("Progress Tracking")) {
		suggestions.push("Consider adding a Progress Tracking section");
	}

	const valid = strict ? errors.length === 0 && warnings.length === 0 : errors.length === 0;

	return { valid, errors, warnings, suggestions };
}

export function getSubtask(
	planContent: string,
	subtaskId: string
): { found: boolean; subtask?: { id: string; title: string; phase: string; task: string } } {
	// Simple parser - looks for subtask pattern X.Y.Z
	const regex = new RegExp(`(\\d+)\\.(\\d+)\\.${subtaskId.split(".")[2]}[:\\s]+(.+)`, "g");
	const match = regex.exec(planContent);

	if (match) {
		return {
			found: true,
			subtask: {
				id: subtaskId,
				title: match[3]?.trim() || "Unknown",
				phase: match[1] || "0",
				task: `${match[1]}.${match[2]}`,
			},
		};
	}

	return { found: false };
}

export function updateProgress(
	planContent: string,
	subtaskId: string,
	completionNotes: string
): string {
	// Add completion marker to subtask
	const timestamp = new Date().toISOString().split("T")[0];
	const marker = `[COMPLETED ${timestamp}] ${completionNotes}`;

	// Find and update the subtask line
	const lines = planContent.split("\n");
	const updatedLines = lines.map((line) => {
		if (line.includes(subtaskId) && !line.includes("[COMPLETED")) {
			return `${line} ${marker}`;
		}
		return line;
	});

	return updatedLines.join("\n");
}

/**
 * Generate an executor agent file for the project.
 * The executor agent is a specialized Haiku-powered agent that executes subtasks
 * with full context of the project's planning documents.
 */
export function generateExecutorAgent(
	briefContent: string,
	language: string = "python"
): { content: string; filePath: string } {
	const brief = parseBrief(briefContent);
	const projectSlug = brief.projectName.toLowerCase().replace(/[^a-z0-9]+/g, "-");
	const projectUnderscore = brief.projectName.toLowerCase().replace(/-/g, "_");
	const isPython = language === "python";

	const filePath = `.claude/agents/${projectSlug}-executor.md`;

	const content = `---
name: ${projectSlug}-executor
description: >
  PROACTIVELY use this agent to execute ${brief.projectName} development subtasks.
  Expert at DEVELOPMENT_PLAN.md execution with cross-checking, git
  discipline, and verification. Invoke with "execute subtask X.Y.Z" to
  complete a subtask entirely in one session.
tools: Read, Write, Edit, Bash, Glob, Grep
model: haiku
---

# ${brief.projectName} Development Plan Executor

## Purpose

Execute development subtasks for **${brief.projectName}** with mechanical precision. Each subtask in the DEVELOPMENT_PLAN.md contains complete, copy-pasteable code that can be implemented without creative inference.

## Project Context

**Project**: ${brief.projectName}
**Type**: ${brief.projectType}
**Goal**: ${brief.primaryGoal}
**Target Users**: ${brief.targetUsers}

**Tech Stack**:
${isPython ? `- Language: Python 3.11+
- CLI Framework: Click
- Testing: pytest + pytest-cov
- Linting: ruff
- Type Checking: mypy` : `- Language: TypeScript
- Testing: Jest/Vitest
- Linting: ESLint + Prettier
- Type Checking: TypeScript strict mode`}

**Directory Structure**:
\`\`\`
${brief.projectName}/
 ${projectUnderscore}/           # Main package
    __init__.py
    cli.py              # CLI commands
    ...                 # Feature modules
 tests/
    test_*.py           # Test modules
 PROJECT_BRIEF.md        # Requirements
 DEVELOPMENT_PLAN.md     # This plan
 CLAUDE.md               # Development rules
\`\`\`

## Haiku-Executable Expectations

Each subtask in the DEVELOPMENT_PLAN.md contains:
- **Complete code blocks** - Copy-pasteable, not pseudocode
- **Explicit file paths** - Exact locations for all files
- **Full imports** - All required imports listed
- **Type hints** - Complete function signatures
- **Verification commands** - Specific commands with expected outputs

## Mandatory Initialization Sequence

Before executing ANY subtask:

1. **Read core documents**:
   - Read CLAUDE.md completely
   - Read DEVELOPMENT_PLAN.md completely
   - Read PROJECT_BRIEF.md for context

2. **Parse the subtask ID** from the prompt (format: X.Y.Z)

3. **Verify prerequisites**:
   - Check that all prerequisite subtasks are marked \`[x]\` complete
   - Read completion notes from prerequisites for context
   - If prerequisites incomplete, STOP and report

4. **Check git state**:
   - Verify correct branch for the TASK (not subtask)
   - Create branch if starting a new task: \`feature/{phase}-{task}-{description}\`

## Execution Protocol

For each subtask:

### 1. Cross-Check Before Writing
- Read existing files that will be modified
- Understand current code patterns
- Verify no conflicts with existing code

### 2. Implement Deliverables
- Complete each deliverable checkbox in order
- Use exact code from DEVELOPMENT_PLAN.md when provided
- Match established patterns in the codebase
- Add type hints to all functions

### 3. Write Tests
- Create tests for all new functions/classes
- Target ${isPython ? "100%" : "high"} coverage on new code
- Test success cases, failures, and edge cases

### 4. Run Verification
\`\`\`bash
${isPython ? `# Linting
ruff check ${projectUnderscore} tests

# Type checking
mypy ${projectUnderscore}

# Tests with coverage
pytest tests/ -v --cov=${projectUnderscore} --cov-report=term-missing` : `# Linting
npm run lint

# Type checking
npm run typecheck

# Tests
npm test`}
\`\`\`

### 5. Update Documentation
- Mark all deliverable checkboxes \`[x]\` complete
- Fill in Completion Notes template with:
  - Implementation summary
  - Files created (with line counts)
  - Files modified
  - Test results and coverage
  - Build verification results

### 6. Commit
\`\`\`bash
git add .
git commit -m "feat(scope): description

- Bullet points of changes
- Test coverage: X%"
\`\`\`

### 7. Merge (if task complete)
When ALL subtasks in a task are done:
\`\`\`bash
git checkout main
git merge --squash feature/{branch-name}
git commit -m "feat: complete task X.Y - description"
git branch -d feature/{branch-name}
\`\`\`

## Git Discipline

**CRITICAL**: Branching is at the TASK level, not subtask level.

- **One branch per TASK** (e.g., \`feature/0-1-repository-setup\`)
- **One commit per SUBTASK** within the task branch
- **Squash merge** when task completes (all subtasks done)
- **Delete branch** after merge

Branch naming: \`feature/{phase}-{task}-{short-description}\`

## Error Handling

If blocked:
1. Do NOT commit broken code
2. Document in DEVELOPMENT_PLAN.md:
   \`\`\`markdown
   **Completion Notes**:
   - **Status**:  BLOCKED
   - **Error**: [Detailed error message]
   - **Attempted**: [What was tried]
   - **Root Cause**: [Analysis]
   - **Suggested Fix**: [What should be done]
   \`\`\`
3. Report immediately to user

## Invocation

To execute a subtask, use:
\`\`\`
Use the ${projectSlug}-executor agent to execute subtask X.Y.Z
\`\`\`

The agent will:
1. Read all planning documents
2. Verify prerequisites
3. Implement the subtask completely
4. Run verification
5. Commit changes
6. Report completion

---

*Generated by DevPlan MCP Server*
`;

	return { content, filePath };
}
